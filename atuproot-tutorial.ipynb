{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AtUproot - tutorial\n",
    "\n",
    "## Concept\n",
    "\n",
    "Take alphatwirl's `BEvents` and change the concept of `events` to `blocks` of events. The `tree` object becomes an `uproot.tree` with `__getattr__` using `uproot.tree.array` to an event attribute for a block of events. The array is then cached for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_branch(self, name):\n",
    "    if name in self._branch_cache:\n",
    "        branch = self._branch_cache[name]\n",
    "    else:\n",
    "        self.entrystart = self.iBlock * self.blocksize\n",
    "        self.entrystop = min((self.iBlock+1) * self.blocksize, self.nEvents)\n",
    "        self.size = self.entrystop - self.entrystart\n",
    "        branch = self.tree.array(name,\n",
    "                                 entrystart = self.entrystart,\n",
    "                                 entrystop = self.entrystop)\n",
    "        self._branch_cache[name] = branch\n",
    "    return branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iteration over the `BEvents` would be over blocks rather than events. The `_branch_cache` is cleared on each iteration of the loop to clear space in memory and for the new events to be read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __iter__(self):\n",
    "    for self.iBlock in range(self.nBlocks):\n",
    "        self._branch_cache = {}\n",
    "        yield self\n",
    "    self.iBlock = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EventBuilder` and `EventBuilderConfigMaker` are adjusted to accommodate the new `BEvents`\n",
    "\n",
    "`AtUproot` is the interface to use `alphatwirl` with the new `BEvents`\n",
    "\n",
    "`Dataset` is updated just for my own personal use\n",
    "\n",
    "Readers are coded in the same way, but the implementation must accommodate the array / JaggedArray format of branches. My choice of implementation is to pass the content, starts and stops of Jagged arrays to a function for jit with numba. Numba can speedup for loops to the same speed as normal numpy functions. However, they are very flexible and quick to write / understand (which I struggle with sometimes with a chain of numpy commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Certified lumi checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, boolean\n",
    "import numpy as np\n",
    "\n",
    "@njit\n",
    "def is_certified_lumi(runs, lumis, cert_runs, cert_lumis):\n",
    "    nev = runs.shape[0]\n",
    "    is_certified = np.ones(nev, dtype=boolean)\n",
    "\n",
    "    for iev in range(nev):\n",
    "        # run not in list, skip\n",
    "        passed = False\n",
    "        for irun in range(cert_runs.shape[0]):\n",
    "            if runs[iev] != cert_runs[irun]:\n",
    "                continue\n",
    "\n",
    "            cert_lumi_range = cert_lumis[irun]\n",
    "            for ibin in range(cert_lumi_range.shape[0]):\n",
    "                if cert_lumi_range[ibin,0] <= lumis[iev] <= cert_lumi_range[ibin,1]:\n",
    "                    passed = True\n",
    "                    break\n",
    "\n",
    "            if passed:\n",
    "                break\n",
    "        is_certified[iev] = passed\n",
    "\n",
    "    return is_certified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection on Jagged array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "\n",
    "@njit\n",
    "def create_new_stops(selection, starts, lens):\n",
    "    nev = lens.shape[0]\n",
    "    new_stops = np.zeros(nev, dtype=int32)\n",
    "\n",
    "    count = 0\n",
    "    for iev in range(nev):\n",
    "        for ij in range(lens[iev]):\n",
    "            rij = starts[iev]+ij\n",
    "            if selection[rij]:\n",
    "                count += 1\n",
    "        new_stops[iev] = count\n",
    "    return new_stops\n",
    "\n",
    "def test_new_stops(self, ref_branch):\n",
    "    new_stops = create_new_stops(\n",
    "        self.selection, ref_branch.starts, ref_branch.stops-ref_branch.starts,\n",
    "    )\n",
    "    new_starts = np.roll(new_stops, 1)\n",
    "    new_starts[0] = 0\n",
    "    \n",
    "    array = uproot.interp.jagged.JaggedArray(\n",
    "        ref_branch.content[self.selection],\n",
    "        new_starts,\n",
    "        new_stops,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Clone the repository, into branch devs for now\n",
    "\n",
    "atuproot comes with a setup script (shamelessly taken from FAST-RA1). I normally have a miniconda environment ready with everything and use that.\n",
    "\n",
    "Here we can just use SWAN's built in ROOT and pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'atuproot' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone -b devs https://github.com/shane-breeze/atuproot atuproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /cvmfs/sft.cern.ch/lcg/views/LCG_93/x86_64-slc6-gcc62-opt/lib/python2.7/site-packages\n",
      "Requirement already satisfied: icc-rt in /eos/user/s/sbreeze/.local/lib/python2.7/site-packages\n",
      "Requirement already satisfied: numba in /eos/user/s/sbreeze/.local/lib/python2.7/site-packages\n",
      "Requirement already satisfied: alphatwirl==0.16.0 in /eos/user/s/sbreeze/.local/lib/python2.7/site-packages\n",
      "Requirement already satisfied: pandas in /cvmfs/sft.cern.ch/lcg/views/LCG_93/x86_64-slc6-gcc62-opt/lib/python2.7/site-packages\n",
      "Requirement already satisfied: pyyaml in /cvmfs/sft.cern.ch/lcg/views/LCG_93/x86_64-slc6-gcc62-opt/lib/python2.7/site-packages\n",
      "Requirement already satisfied: uproot in /eos/user/s/sbreeze/.local/lib/python2.7/site-packages\n",
      "Requirement already satisfied: rootpy in /cvmfs/sft.cern.ch/lcg/views/LCG_93/x86_64-slc6-gcc62-opt/lib/python2.7/site-packages\n",
      "Requirement already satisfied: intel-openmp==2018.0.3 in /eos/user/s/sbreeze/.local/lib/python2.7/site-packages (from icc-rt)\n",
      "Requirement already satisfied: funcsigs in /cvmfs/sft.cern.ch/lcg/views/LCG_93/x86_64-slc6-gcc62-opt/lib/python2.7/site-packages (from numba)\n",
      "Requirement already satisfied: enum34 in /cvmfs/sft.cern.ch/lcg/views/LCG_93/x86_64-slc6-gcc62-opt/lib/python2.7/site-packages (from numba)\n",
      "Requirement already satisfied: singledispatch in /cvmfs/sft.cern.ch/lcg/views/LCG_93/x86_64-slc6-gcc62-opt/lib/python2.7/site-packages (from numba)\n",
      "Requirement already satisfied: llvmlite>=0.24.0dev0 in /eos/user/s/sbreeze/.local/lib/python2.7/site-packages (from numba)\n",
      "Requirement already satisfied: python-dateutil in /cvmfs/sft.cern.ch/lcg/views/LCG_93/x86_64-slc6-gcc62-opt/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2011k in /cvmfs/sft.cern.ch/lcg/views/LCG_93/x86_64-slc6-gcc62-opt/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied: six in /cvmfs/sft.cern.ch/lcg/views/LCG_93/x86_64-slc6-gcc62-opt/lib/python2.7/site-packages (from singledispatch->numba)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user numpy icc-rt numba alphatwirl==0.16.0 pandas pyyaml uproot rootpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"atuproot/\")\n",
    "sys.path.append(\"atuproot/atuproot/\")\n",
    "sys.path.append(\"atuproot/sequence/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the code\n",
    "\n",
    "There is a single script to run the atuproot interface to alphatwirl:\n",
    "`python run.py --blocksize 100000 --mode multiprocessing`\n",
    "\n",
    "The following will do what's done inside the `run.py` script (without using a command line parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(name = 'MET_Run2016B_v2', parent = 'MET', isdata = True, xsection = None, lumi = 5930, energy = 13000, sumweights = 35987712.0, associates = MET_Run2016B_v2)\n",
      "Dataset(name = 'ZJetsToNuNu_Pt-250To400', parent = 'ZJetsToNuNu', isdata = False, xsection = 6.219, lumi = 5930, energy = 13000, sumweights = 5163758.924870491, associates = ZJetsToNuNu_Pt-250To400)\n"
     ]
    }
   ],
   "source": [
    "from datasets.datasets import get_datasets\n",
    "datasets = get_datasets(path=\"atuproot/datasets/datasets.yaml\")\n",
    "for d in datasets:\n",
    "    print d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.12/06\n"
     ]
    }
   ],
   "source": [
    "from atuproot.AtUproot import AtUproot\n",
    "process = AtUproot(\"output\",\n",
    "                  quiet = False,\n",
    "                  parallel_mode = \"multiprocessing\",\n",
    "                  process = 4,\n",
    "                  max_blocks_per_dataset = -1,\n",
    "                  max_blocks_per_process = -1,\n",
    "                  blocksize = 500000,\n",
    "                  profile = False,\n",
    "                  profile_out_path = \"profile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence.Readers import ScribblerWrapper\n",
    "from sequence.sequence import sequence\n",
    "reader_collector_pairs = [(ScribblerWrapper(rc[0]), rc[1]) for rc in sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sequence.Readers.CollectionCreator.CollectionCreator object at 0x7f26a625dbd0>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.GenBosonProducer.GenBosonProducer object at 0x7f26a00d4390>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.JecVariations.JecVariations object at 0x7f26a01ddb50>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.SkimCollections.SkimCollections object at 0x7f26a01de150>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.ObjectCrossCleaning.ObjectCrossCleaning object at 0x7f26a01de210>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.ObjectCrossCleaning.ObjectCrossCleaning object at 0x7f26a01deb10>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.EventSumsProducer.EventSumsProducer object at 0x7f26a00d4190>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.InvMassProducer.InvMassProducer object at 0x7f26a00d4350>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.TriggerChecker.TriggerChecker object at 0x7f26a625dc10>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.CertifiedLumiChecker.CertifiedLumiChecker object at 0x7f26a625d3d0>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.SignalRegionBlinder.SignalRegionBlinder object at 0x7f26a00d4310>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.SelectionProducer.SelectionProducer object at 0x7f26a00d4510>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.WeightCreator.WeightCreator object at 0x7f26a00d43d0>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.WeightXsLumi.WeightXsLumi object at 0x7f26a00d4410>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.WeightPileup.WeightPileup object at 0x7f26a00d4450>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.WeightMetTrigger.WeightMetTrigger object at 0x7f26a00d4490>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Readers.WeightMuons.WeightMuons object at 0x7f26a00d44d0>\n",
      "NullCollector()\n",
      "\n",
      "<sequence.Collectors.Histogrammer.HistReader object at 0x7f26a00d4550>\n",
      "<sequence.Collectors.Histogrammer.HistCollector object at 0x7f26a00d4590>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rc in reader_collector_pairs:\n",
    "    print rc[0].scribbler\n",
    "    print rc[1]\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fri Aug 10 00:12:15 2018\n",
      "        0 /       16 (  0.00%) MET_Run2016B_v2 \n",
      "\n",
      "Fri Aug 10 00:12:15 2018\n",
      "        0 /       16 (  0.00%) MET_Run2016B_v2 \n",
      "        0 /       17 (  0.00%) MET_Run2016B_v2 \n",
      "\n",
      "Fri Aug 10 00:12:16 2018\n",
      "        0 /       16 (  0.00%) MET_Run2016B_v2 \n",
      "        0 /       17 (  0.00%) MET_Run2016B_v2 \n",
      "        0 /       16 (  0.00%) MET_Run2016B_v2 \n",
      "\n",
      "Fri Aug 10 00:12:16 2018\n",
      "        0 /       16 (  0.00%) MET_Run2016B_v2 \n",
      "        0 /       17 (  0.00%) MET_Run2016B_v2 \n",
      "        0 /       16 (  0.00%) MET_Run2016B_v2 \n",
      "        0 /        6 (  0.00%) MET_Run2016B_v2 \n",
      "\n",
      "Fri Aug 10 00:13:17 2018\n",
      "        8 /       16 ( 50.00%) MET_Run2016B_v2 \n",
      "        8 /       17 ( 47.06%) MET_Run2016B_v2 \n",
      "        3 /       16 ( 18.75%) MET_Run2016B_v2 \n",
      "        5 /        6 ( 83.33%) MET_Run2016B_v2 \n",
      "\n",
      "Fri Aug 10 00:13:18 2018\n",
      "        6 /        6 (100.00%) MET_Run2016B_v2 \n",
      "        8 /       16 ( 50.00%) MET_Run2016B_v2 \n",
      "        8 /       17 ( 47.06%) MET_Run2016B_v2 \n",
      "        3 /       16 ( 18.75%) MET_Run2016B_v2 \n"
     ]
    }
   ],
   "source": [
    "process.run(datasets, reader_collector_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "from atuproot.BEvents import BEvents\n",
    "events = BEvents(uproot.open(datasets[0].files[0])[\"Events\"], blocksize=100, maxBlocks=1, start=0)\n",
    "event = events[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.Jet_pt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CertifiedLumiChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence.sequence import certified_lumi_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load atuproot/sequence/Readers/CertifiedLumiChecker.py\n",
    "import json\n",
    "import numpy as np\n",
    "from numba import njit, boolean\n",
    "\n",
    "class CertifiedLumiChecker(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def begin(self, event):\n",
    "        self.runs, self.lumi_list = read_json(self.lumi_json_path)\n",
    "\n",
    "    def event(self, event):\n",
    "        event.IsCertified = is_certified_lumi(event.run, event.luminosityBlock,\n",
    "                                              self.runs, self.lumi_list)\n",
    "\n",
    "@njit\n",
    "def is_certified_lumi(runs, lumis, cert_runs, cert_lumis):\n",
    "    nev = runs.shape[0]\n",
    "    is_certified = np.ones(nev, dtype=boolean)\n",
    "\n",
    "    for iev in range(nev):\n",
    "        # run not in list, skip\n",
    "        passed = False\n",
    "        for irun in range(cert_runs.shape[0]):\n",
    "            if runs[iev] != cert_runs[irun]:\n",
    "                continue\n",
    "\n",
    "            cert_lumi_range = cert_lumis[irun]\n",
    "            for ibin in range(cert_lumi_range.shape[0]):\n",
    "                if cert_lumi_range[ibin,0] <= lumis[iev] <= cert_lumi_range[ibin,1]:\n",
    "                    passed = True\n",
    "                    break\n",
    "\n",
    "            if passed:\n",
    "                break\n",
    "        is_certified[iev] = passed\n",
    "\n",
    "    return is_certified\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    runs = np.array(sorted(map(int, data.keys())))\n",
    "    lumis = [np.array(data[str(r)], dtype=int) for r in runs]\n",
    "    return runs, lumis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "certified_lumi_checker.begin(event)\n",
    "certified_lumi_checker.event(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print event.IsCertified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence.sequence import collection_creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_creator.event(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print event.Jet # Collection(name, ref_name, selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print event.Jet.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skim collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence.sequence import skim_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skim_collections.begin(event)\n",
    "skim_collections.event(event)\n",
    "skim_collections.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print event.JetSelection # Collection(name, ref_name, selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print event.JetSelection_pt\n",
    "except AttributeError:\n",
    "    print \"No attribute JetSelection_pt\"\n",
    "print event.JetSelection.pt\n",
    "print event.JetSelection_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A selection is applied to each object in a collection resulting in a boolean array\n",
    "    * Selections are applied as logical operations between the contents of jagged arrays: e.g. `(jet.pt > 40.) & (abs(jet.eta) < 2.4)`\n",
    "* This array is stored in the new collection along with the reference collection's name. No new arrays are created\n",
    "* When an attribute of the new collection is called:\n",
    "    * the boolean array is applied to the reference collection's attribute\n",
    "    * new starts and stops are generated\n",
    "    * a jagged array is created and cached for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence.sequence import jet_cross_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_cross_cleaning.event(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.JetSelection.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Jets are removed if they overlap with muons, electron or photons with a DeltaR cone of 0.4\n",
    "* The JetSelection collection's boolean array is updated to reject these jets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atuproot.EventBuilderConfigMaker import EventBuilderConfig\n",
    "\n",
    "config = EventBuilderConfig(\n",
    "    inputPaths = [datasets[0].files[0]],\n",
    "    treeName = \"Events\",\n",
    "    maxBlocks = 1,\n",
    "    start = 0,\n",
    "    blocksize = 10000,\n",
    "    dataset = datasets[0],\n",
    "    name = datasets[0].name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = BEvents(uproot.open(datasets[0].files[0])[\"Events\"], blocksize=10000, maxBlocks=1, start=0)\n",
    "event = events[0]\n",
    "event.config = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rc in reader_collector_pairs:\n",
    "    rc[0].begin(event)\n",
    "    rc[0].event(event)\n",
    "    if hasattr(rc[0], \"end\"):\n",
    "        rc[0].end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event.Cutflow_Monojet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `SelectionProducer` takes a list of cuts (takes the logical and of all these selections applied to the event) and creates a boolean array\n",
    "    * The selections are applied as logical operations between arrays\n",
    "* The selection is not applied to the event and should be used with reader-collector pairs allowing for multiple cutflows in 1 run\n",
    "* If the selection reduces the number of events to a small enough value, these events can be exported into dataframes for further manipulation / plotting (e.g. machine learning\n",
    "* Currently I have a collector which histograms the arrays and then draws the distributions (standard alphatwirl aggregation tools should be fairly simple to adopt with atuproot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print event.METnoX.pt\n",
    "print event.METnoX.pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print event.Cutflow_Monojet\n",
    "print event.Cutflow_Monojet.shape\n",
    "print np.argwhere(event.Cutflow_Monojet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print event.METnoX.pt[event.Cutflow_Monojet]\n",
    "print event.METnoX.pt[event.Cutflow_Monojet].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print event.METnoX.pt[1821]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Can use up a lot of memory\n",
    "    1. If branches are only accessed in one reader and not in any others then theres a function in `BEvents` allowing the deletion of branches from the cache\n",
    "    1. `blocksize` can be reduced to load fewer event in per block. This can reduce the performance if you ask for too few\n",
    "    1. Some samples use more memory than others. Request more memory for those samples if possible\n",
    "    1. Profile the memory usage with `memory_profiler`\n",
    "1. No support for chaining files (yet)\n",
    "    1. So far there's not support for chaining files. The number of files per process should always be 1\n",
    "    1. I've hadded my root files (into several larger files). ~300-400 files altogher = 300-400 jobs\n",
    "1. Speed\n",
    "    1. Profiling the code shows that the major bottleneck is reading in the trees with uproot (even if it is really fast)\n",
    "    1. lzma compression slows this reading process down. If possible lower the compression of your files saving uproot to do it on each run\n",
    "    1. Good side of this is that alphatwirl's readers / collectors don't impact the speed significantly. Especially with the use of numba's jit compliation\n",
    "1. Reading large root files\n",
    "    1. `uproot` creates a memory mapping of root files. Therefore files larges than ~4 GB can't be mapped on 32-bit systems (if anyone is usings these still?)\n",
    "    1. `ulimit -v` can also limit this. Especially annoying if the cluster has a hard limit on the maximum vitrual memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
